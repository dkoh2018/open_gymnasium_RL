{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from collections import deque, namedtuple\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "going to cover a small basic rundown of what you need to do for reinforcement learning. obviously tons of resources online as well\n",
    "\n",
    "each step should help you understand the concept as well as learning to translate it into syntax\n",
    "\n",
    "1) Environment : the system in which the agent will interact (we'll use the lunar landing)\n",
    "2) Agent : entity making decisions to take action within env\n",
    "3) State : representing the current situation of the env ( i.e. velocity and position of our lunar lander)\n",
    "4) Action : decisions made by the agent ( i.e. angle and thrust)\n",
    "5) Reward : feedback from the env based on actions ( i.e. give points for succesfully landing)\n",
    "6) Policy : what strategy will the agent use to decide actions based on states?\n",
    "\n",
    "\n",
    "\n",
    "So to build one:\n",
    "1) Define your neural network : network will approx the policy or the value function\n",
    "2) Create an experience replay memory: store expeirences (your state, action, reward, next state) to sample from training the network\n",
    "3) Implement training loop : interact with env, store experiences, sample from memory, and continue to update network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CarRacing-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "\n",
    "    def store(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        if len(self.buffer) > self.capacity:\n",
    "            self.buffer.pop(0)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        batch = random.sample(self.buffer, k=batch_size)\n",
    "        states = torch.from_numpy(np.vstack([exp[0] for exp in batch if exp is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([exp[1] for exp in batch if exp is not None])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([exp[2] for exp in batch if exp is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([exp[3] for exp in batch if exp is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([exp[4] for exp in batch if exp is not None]).astype(np.uint8)).float().to(self.device)\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
